{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN6vhf1sR95l",
        "outputId": "5f9443eb-a0e0-4f89-a407-43ef5357ce55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: judgeval==0.0.51 in /opt/anaconda3/lib/python3.11/site-packages (0.0.51)\n",
            "Requirement already satisfied: graphiti-core in /opt/anaconda3/lib/python3.11/site-packages (0.15.1)\n",
            "Requirement already satisfied: neo4j in /opt/anaconda3/lib/python3.11/site-packages (5.28.1)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.11/site-packages (0.5.1)\n",
            "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.93.2)\n",
            "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.11/site-packages (0.3.18)\n",
            "Requirement already satisfied: anthropic in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (0.57.1)\n",
            "Requirement already satisfied: boto3 in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.39.3)\n",
            "Requirement already satisfied: google-genai in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.20.0)\n",
            "Requirement already satisfied: langchain-anthropic in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (0.3.17)\n",
            "Requirement already satisfied: langchain-core in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (0.3.68)\n",
            "Requirement already satisfied: langchain-huggingface in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (0.3.0)\n",
            "Requirement already satisfied: langchain-openai in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (0.2.3)\n",
            "Requirement already satisfied: litellm==1.61.15 in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.61.15)\n",
            "Requirement already satisfied: matplotlib>=3.10.3 in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (3.10.3)\n",
            "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.6.0)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (2.32.3)\n",
            "Requirement already satisfied: together in /opt/anaconda3/lib/python3.11/site-packages (from judgeval==0.0.51) (1.5.19)\n",
            "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (3.11.16)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (8.1.7)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (8.4.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (4.24.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (2.11.7)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (0.8.0)\n",
            "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.11/site-packages (from litellm==1.61.15->judgeval==0.0.51) (0.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.61.15->judgeval==0.0.51) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.61.15->judgeval==0.0.51) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.61.15->judgeval==0.0.51) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.61.15->judgeval==0.0.51) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.61.15->judgeval==0.0.51) (0.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.61.15->judgeval==0.0.51) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.61.15->judgeval==0.0.51) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.61.15->judgeval==0.0.51) (4.12.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->litellm==1.61.15->judgeval==0.0.51) (0.4.0)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /opt/anaconda3/lib/python3.11/site-packages (from graphiti-core) (5.6.3)\n",
            "Requirement already satisfied: numpy>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from graphiti-core) (1.26.4)\n",
            "Requirement already satisfied: posthog>=3.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from graphiti-core) (3.7.0)\n",
            "Requirement already satisfied: tenacity>=9.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from graphiti-core) (9.1.2)\n",
            "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.11/site-packages (from neo4j) (2024.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (0.5.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (0.1.72)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /opt/anaconda3/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.61.15->judgeval==0.0.51) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.61.15->judgeval==0.0.51) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.61.15->judgeval==0.0.51) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->litellm==1.61.15->judgeval==0.0.51) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core->judgeval==0.0.51) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-core->judgeval==0.0.51) (23.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->judgeval==0.0.51) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->judgeval==0.0.51) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->judgeval==0.0.51) (2.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.61.15->judgeval==0.0.51) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (1.4.4)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.10.3->judgeval==0.0.51) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from posthog>=3.0.0->graphiti-core) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from posthog>=3.0.0->graphiti-core) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from posthog>=3.0.0->graphiti-core) (2.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.61.15->judgeval==0.0.51) (2024.9.11)\n",
            "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.11/site-packages (from zipp>=0.5->importlib-metadata>=6.8.0->litellm==1.61.15->judgeval==0.0.51) (8.10.0)\n",
            "Requirement already satisfied: botocore<1.40.0,>=1.39.3 in /opt/anaconda3/lib/python3.11/site-packages (from boto3->judgeval==0.0.51) (1.39.3)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from boto3->judgeval==0.0.51) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from boto3->judgeval==0.0.51) (0.13.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai->judgeval==0.0.51) (2.35.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-genai->judgeval==0.0.51) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai->judgeval==0.0.51) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai->judgeval==0.0.51) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai->judgeval==0.0.51) (4.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai->judgeval==0.0.51) (0.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /opt/anaconda3/lib/python3.11/site-packages (from langchain-huggingface->judgeval==0.0.51) (0.33.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface->judgeval==0.0.51) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface->judgeval==0.0.51) (2024.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.30.2->langchain-huggingface->judgeval==0.0.51) (1.1.2)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /opt/anaconda3/lib/python3.11/site-packages (from together->judgeval==0.0.51) (0.2.2)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from together->judgeval==0.0.51) (13.9.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/anaconda3/lib/python3.11/site-packages (from together->judgeval==0.0.51) (0.9.0)\n",
            "Requirement already satisfied: typer<0.16,>=0.9 in /opt/anaconda3/lib/python3.11/site-packages (from together->judgeval==0.0.51) (0.12.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<15.0.0,>=13.8.1->together->judgeval==0.0.51) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<15.0.0,>=13.8.1->together->judgeval==0.0.51) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<0.16,>=0.9->together->judgeval==0.0.51) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.8.1->together->judgeval==0.0.51) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install judgeval==0.0.51 graphiti-core neo4j pandas langgraph openai langchain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ASeZyzHYnWa"
      },
      "source": [
        "# **IMPORT NECESSARY PACKAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afv4ts-2YnWb"
      },
      "outputs": [],
      "source": [
        "# Standard Library\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import uuid\n",
        "import asyncio\n",
        "import logging\n",
        "import warnings\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from typing import TypedDict, Sequence, Annotated\n",
        "# Environment Variables\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# IPython/Jupyter\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, START, END, add_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# LangChain\n",
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Judgeval\n",
        "from judgeval.judgment_client import JudgmentClient\n",
        "from judgeval.scorers.judgeval_scorers.api_scorers import FaithfulnessScorer\n",
        "from judgeval.common.tracer import Tracer\n",
        "from judgeval.integrations.langgraph import JudgevalCallbackHandler\n",
        "from judgeval.data import Example\n",
        "from judgeval.data.datasets import EvalDataset\n",
        "from judgeval.scorers import AnswerCorrectnessScorer\n",
        "from judgeval.scorers import InstructionAdherenceScorer\n",
        "\n",
        "# Graphiti\n",
        "from graphiti_core import Graphiti\n",
        "from graphiti_core.nodes import EpisodeType\n",
        "from graphiti_core.edges import EntityEdge\n",
        "from graphiti_core.utils.maintenance.graph_data_operations import clear_data\n",
        "from graphiti_core.search.search_config_recipes import NODE_HYBRID_SEARCH_EPISODE_MENTIONS\n",
        "\n",
        "# Misc\n",
        "import warnings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4gLp5m4YnWb"
      },
      "source": [
        "# **INITIALIZE OPENAI API KEY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUNtYfW_YnWb"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Z4TQ_STYnWc"
      },
      "source": [
        "# **READ THE USDA FOOD DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXvPCMIWSHTB",
        "outputId": "baad9dc5-92ec-4d9e-fa01-14507e3b8446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "food_calorie_conversion_factor.csv:\n",
            "['food_nutrient_conversion_factor_id', 'protein_value', 'fat_value', 'carbohydrate_value']\n",
            "--------------------------------------------------\n",
            "food_nutrient_conversion_factor.csv:\n",
            "['id', 'fdc_id']\n",
            "--------------------------------------------------\n",
            "food_attribute_type.csv:\n",
            "['id', 'name', 'description']\n",
            "--------------------------------------------------\n",
            "food_attribute.csv:\n",
            "['id', 'fdc_id', 'seq_num', 'food_attribute_type_id', 'name', 'value']\n",
            "--------------------------------------------------\n",
            "market_acquisition.csv:\n",
            "['fdc_id', 'brand_description', 'expiration_date', 'label_weight', 'location', 'acquisition_date', 'sales_type', 'sample_lot_nbr', 'sell_by_date', 'store_city', 'store_name', 'store_state', 'upc_code']\n",
            "--------------------------------------------------\n",
            "foundation_food.csv:\n",
            "['fdc_id', 'NDB_number', 'footnote']\n",
            "--------------------------------------------------\n",
            "food_update_log_entry.csv:\n",
            "['id', 'description', 'last_updated']\n",
            "--------------------------------------------------\n",
            "measure_unit.csv:\n",
            "['id', 'name']\n",
            "--------------------------------------------------\n",
            "sub_sample_result.csv:\n",
            "['food_nutrient_id', 'adjusted_amount', 'lab_method_id', 'nutrient_name']\n",
            "--------------------------------------------------\n",
            "nutrient.csv:\n",
            "['id', 'name', 'unit_name', 'nutrient_nbr', 'rank']\n",
            "--------------------------------------------------\n",
            "food.csv:\n",
            "['fdc_id', 'data_type', 'description', 'food_category_id', 'publication_date']\n",
            "--------------------------------------------------\n",
            "lab_method_nutrient.csv:\n",
            "['lab_method_id', 'nutrient_id']\n",
            "--------------------------------------------------\n",
            "food_nutrient.csv:\n",
            "['id', 'fdc_id', 'nutrient_id', 'amount', 'data_points', 'derivation_id', 'min', 'max', 'median', 'footnote', 'min_year_acquired']\n",
            "--------------------------------------------------\n",
            "sample_food.csv:\n",
            "['fdc_id']\n",
            "--------------------------------------------------\n",
            "input_food.csv:\n",
            "['id', 'fdc_id', 'fdc_of_input_food', 'seq_num', 'amount', 'ingredient_code', 'ingredient_description', 'unit', 'portion_code', 'portion_description', 'gram_weight', 'retention_code']\n",
            "--------------------------------------------------\n",
            "lab_method_code.csv:\n",
            "['lab_method_id', 'code']\n",
            "--------------------------------------------------\n",
            "acquisition_samples.csv:\n",
            "['fdc_id_of_sample_food', 'fdc_id_of_acquisition_food']\n",
            "--------------------------------------------------\n",
            "food_protein_conversion_factor.csv:\n",
            "['food_nutrient_conversion_factor_id', 'value']\n",
            "--------------------------------------------------\n",
            "sub_sample_food.csv:\n",
            "['fdc_id', 'fdc_id_of_sample_food']\n",
            "--------------------------------------------------\n",
            "agricultural_samples.csv:\n",
            "['fdc_id', 'acquisition_date', 'market_class', 'treatment', 'state']\n",
            "--------------------------------------------------\n",
            "food_portion.csv:\n",
            "['id', 'fdc_id', 'seq_num', 'amount', 'measure_unit_id', 'portion_description', 'modifier', 'gram_weight', 'data_points', 'footnote', 'min_year_acquired']\n",
            "--------------------------------------------------\n",
            "food_component.csv:\n",
            "['id', 'fdc_id', 'name', 'pct_weight', 'is_refuse', 'gram_weight', 'data_points', 'min_year_acqured']\n",
            "--------------------------------------------------\n",
            "lab_method.csv:\n",
            "['id', 'description', 'technique']\n",
            "--------------------------------------------------\n",
            "food_category.csv:\n",
            "['id', 'code', 'description']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Path to the USDA-Food-Dataset folder\n",
        "folder_path = \"USDA-Food-Dataset\"\n",
        "\n",
        "# List all CSV files in the folder\n",
        "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "\n",
        "# Loop through each CSV and print its columns\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, nrows=0)\n",
        "        print(f\"{file}:\")\n",
        "        print(list(df.columns))\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to read {file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKGJNZyzYnWc"
      },
      "source": [
        "# **CREATE KNOWLEDGE GRAPH TRIPLETS FROM THE USDA FOOD DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg14aRffTl5q",
        "outputId": "f03bbc19-aa40-4bb5-969d-2bfb53f3f682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 263941 triplets and saved to usda_full_kg_triplets.json\n"
          ]
        }
      ],
      "source": [
        "folder_path = \"USDA-Food-Dataset/\"\n",
        "\n",
        "# Function to safely read CSV with low_memory=False\n",
        "def read_csv_safe(path):\n",
        "    return pd.read_csv(path, low_memory=False)\n",
        "\n",
        "# Load all CSVs\n",
        "food = read_csv_safe(folder_path + \"food.csv\")\n",
        "food_nutrient = read_csv_safe(folder_path + \"food_nutrient.csv\")\n",
        "nutrient = read_csv_safe(folder_path + \"nutrient.csv\")\n",
        "food_category = read_csv_safe(folder_path + \"food_category.csv\")\n",
        "food_component = read_csv_safe(folder_path + \"food_component.csv\")\n",
        "food_attribute = read_csv_safe(folder_path + \"food_attribute.csv\")\n",
        "food_attribute_type = read_csv_safe(folder_path + \"food_attribute_type.csv\")\n",
        "food_portion = read_csv_safe(folder_path + \"food_portion.csv\")\n",
        "measure_unit = read_csv_safe(folder_path + \"measure_unit.csv\")\n",
        "food_calorie_conv = read_csv_safe(folder_path + \"food_calorie_conversion_factor.csv\")\n",
        "food_protein_conv = read_csv_safe(folder_path + \"food_protein_conversion_factor.csv\")\n",
        "food_nutrient_conv = read_csv_safe(folder_path + \"food_nutrient_conversion_factor.csv\")\n",
        "market_acquisition = read_csv_safe(folder_path + \"market_acquisition.csv\")\n",
        "input_food = read_csv_safe(folder_path + \"input_food.csv\")\n",
        "foundation_food = read_csv_safe(folder_path + \"foundation_food.csv\")\n",
        "\n",
        "triplets = []\n",
        "\n",
        "# Helper function to get description safely\n",
        "def get_desc(row):\n",
        "    return row.get(\"description\", row.get(\"description_x\", \"Unknown Food\"))\n",
        "\n",
        "#  FOOD–NUTRIENT RELATIONSHIPS\n",
        "\n",
        "merged_fn = food_nutrient.merge(food, on=\"fdc_id\").merge(nutrient, left_on=\"nutrient_id\", right_on=\"id\")\n",
        "for _, row in merged_fn.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    nutrient_name = row[\"name\"]\n",
        "    triplets.append( (food_name, \"contains\", nutrient_name) )\n",
        "    if nutrient_name == \"Energy\":\n",
        "        triplets.append( (food_name, \"has calorie value\", f\"{row['amount']} {row['unit_name']}\") )\n",
        "\n",
        "#  FOOD–CATEGORY RELATIONSHIPS\n",
        "\n",
        "merged_fc = food.merge(food_category, left_on=\"food_category_id\", right_on=\"id\", how=\"left\")\n",
        "for _, row in merged_fc.iterrows():\n",
        "    food_name = row.get(\"description_x\", \"Unknown Food\")\n",
        "    category_name = row.get(\"description_y\")\n",
        "    if pd.notna(category_name):\n",
        "        triplets.append( (food_name, \"belongs to category\", category_name) )\n",
        "\n",
        "#  FOOD–COMPONENT RELATIONSHIPS\n",
        "\n",
        "merged_comp = food_component.merge(food, on=\"fdc_id\")\n",
        "for _, row in merged_comp.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    triplets.append( (food_name, \"is source of\", row[\"name\"]) )\n",
        "\n",
        "#  FOOD–ATTRIBUTE RELATIONSHIPS\n",
        "\n",
        "merged_attr = food_attribute.merge(food, on=\"fdc_id\").merge(food_attribute_type, left_on=\"food_attribute_type_id\", right_on=\"id\")\n",
        "for _, row in merged_attr.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    attr_type = row[\"name_y\"]\n",
        "    attr_value = row[\"value\"]\n",
        "    triplets.append( (food_name, f\"has attribute ({attr_type})\", attr_value) )\n",
        "\n",
        "#  FOOD–PORTION RELATIONSHIPS\n",
        "\n",
        "merged_portion = food_portion.merge(food, on=\"fdc_id\").merge(measure_unit, left_on=\"measure_unit_id\", right_on=\"id\")\n",
        "for _, row in merged_portion.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    portion_desc = f\"{row['amount']} {row['name']} ({row['portion_description']})\"\n",
        "    triplets.append( (food_name, \"has portion\", portion_desc) )\n",
        "\n",
        "#  FOOD–CALORIE CONVERSION FACTOR\n",
        "\n",
        "for _, row in food_calorie_conv.iterrows():\n",
        "    factor_id = f\"ConversionFactor-{row['food_nutrient_conversion_factor_id']}\"\n",
        "    triplets.append( (factor_id, \"has protein value\", row[\"protein_value\"]) )\n",
        "    triplets.append( (factor_id, \"has fat value\", row[\"fat_value\"]) )\n",
        "    triplets.append( (factor_id, \"has carbohydrate value\", row[\"carbohydrate_value\"]) )\n",
        "\n",
        "#  FOOD–PROTEIN & NUTRIENT CONVERSION FACTORS\n",
        "\n",
        "for _, row in food_protein_conv.iterrows():\n",
        "    factor_id = f\"ConversionFactor-{row['food_nutrient_conversion_factor_id']}\"\n",
        "    triplets.append( (factor_id, \"has protein conversion factor\", row[\"value\"]) )\n",
        "\n",
        "for _, row in food_nutrient_conv.iterrows():\n",
        "    triplets.append( (row[\"fdc_id\"], \"has nutrient conversion factor id\", row[\"id\"]) )\n",
        "\n",
        "#  MARKET ACQUISITION RELATIONSHIPS\n",
        "\n",
        "merged_market = market_acquisition.merge(food, on=\"fdc_id\")\n",
        "for _, row in merged_market.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    if pd.notna(row.get(\"location\")):\n",
        "        triplets.append( (food_name, \"acquired from location\", row[\"location\"]) )\n",
        "    if pd.notna(row.get(\"brand_description\")):\n",
        "        triplets.append( (food_name, \"has brand\", row[\"brand_description\"]) )\n",
        "\n",
        "#  INPUT FOOD RELATIONSHIPS\n",
        "\n",
        "merged_input = input_food.merge(food, left_on=\"fdc_of_input_food\", right_on=\"fdc_id\", suffixes=('_input', '_food'))\n",
        "for _, row in merged_input.iterrows():\n",
        "    food_name = row.get(\"description_food\", \"Unknown Food\")\n",
        "    input_food_name = row.get(\"description_input\", \"Unknown Input Food\")\n",
        "    triplets.append( (food_name, \"has input food\", input_food_name) )\n",
        "\n",
        "#  FOUNDATION FOOD NDB NUMBERS\n",
        "\n",
        "merged_foundation = foundation_food.merge(food, on=\"fdc_id\")\n",
        "for _, row in merged_foundation.iterrows():\n",
        "    food_name = get_desc(row)\n",
        "    triplets.append( (food_name, \"has NDB number\", row[\"NDB_number\"]) )\n",
        "\n",
        "#  FINAL OUTPUT\n",
        "\n",
        "triplet_df = pd.DataFrame(triplets, columns=[\"Subject\", \"Predicate\", \"Object\"])\n",
        "\n",
        "\n",
        "# Save as pretty JSON\n",
        "output_path_json = \"usda_full_kg_triplets.json\"\n",
        "triplet_df.to_json(output_path_json, orient=\"records\", indent=2)\n",
        "print(f\"Extracted {len(triplets)} triplets and saved to {output_path_json}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRnknCIakGre",
        "outputId": "8132ce4a-5dfb-4939-bf30-088a1a80f236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Unique relationship types:\n",
            "contains\n",
            "has calorie value\n",
            "belongs to category\n",
            "has attribute (Common Name)\n",
            "has attribute (Attribute)\n",
            "has portion\n",
            "has protein value\n",
            "has fat value\n",
            "has carbohydrate value\n",
            "has protein conversion factor\n",
            "has nutrient conversion factor id\n",
            "acquired from location\n",
            "has brand\n",
            "has input food\n",
            "has NDB number\n"
          ]
        }
      ],
      "source": [
        "unique_predicates = triplet_df[\"Predicate\"].unique()\n",
        "\n",
        "print(\"\\nUnique relationship types:\")\n",
        "for rel in unique_predicates:\n",
        "    print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWxDcdZHYnWd"
      },
      "source": [
        "# **SET UP LOGGING AND CONFIGURE GRAPHITI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InKGnoDZzl_w",
        "outputId": "92991ddc-5225-4bf4-dbbc-e221c012d2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root - INFO - Logging setup completed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root - INFO - Connected to Neo4j via Graphiti!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configure logging\n",
        "def setup_logging():\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Avoid adding multiple handlers if re-running in Jupyter\n",
        "    if not logger.handlers:\n",
        "        console_handler = logging.StreamHandler(sys.stdout)\n",
        "        console_handler.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
        "        console_handler.setFormatter(formatter)\n",
        "        logger.addHandler(console_handler)\n",
        "\n",
        "    # Suppress noisy loggers\n",
        "    noisy_loggers = [\n",
        "        \"httpx\",\n",
        "        \"openai\",\n",
        "        \"neo4j\",\n",
        "        \"neo4j.notifications\",\n",
        "        \"graphiti_core\",\n",
        "        \"asyncio\",\n",
        "        \"urllib3\",\n",
        "    ]\n",
        "    for nl in noisy_loggers:\n",
        "        logging.getLogger(nl).setLevel(logging.ERROR)\n",
        "\n",
        "    return logger\n",
        "\n",
        "logger = setup_logging()\n",
        "\n",
        "# Confirm logger initialization\n",
        "logger.info(\"Logging setup completed.\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize Graphiti client\n",
        "client = Graphiti(\n",
        "    \"bolt://127.0.0.1:7687\",\n",
        "    \"neo4j\",\n",
        "    \"HayaVadana11*18\"\n",
        ")\n",
        "\n",
        "logger.info(\"Connected to Neo4j via Graphiti!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hno-DEwOYnWe"
      },
      "source": [
        "# **GENERATING A DATABASE SCHEMA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMvgJ1CCYnWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "#await clear_data(client.driver).   -> Done only the first time, before ingesting triplets\n",
        "await client.build_indices_and_constraints()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQzyN_lYnWe"
      },
      "source": [
        "# **INGESTING THE KNOWLEDGE GRAPH TRIPLETS FOR CONTEXT RETRIEVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM2fTOd5YnWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "async def ingest_usda_triplets(client, batch_size=10):\n",
        "    script_dir = Path.cwd()\n",
        "    json_file_path = script_dir / 'usda_full_kg_triplets.json'\n",
        "    with open(json_file_path) as file:\n",
        "        triplets = json.load(file)\n",
        "        triplets = triplets[:5000]\n",
        "\n",
        "    total = len(triplets)\n",
        "\n",
        "    with tqdm(\n",
        "        total=total,\n",
        "        desc=\"Ingesting USDA Triplets\",\n",
        "        bar_format='{desc}: {percentage:3.0f}% |{bar}|'\n",
        "    ) as pbar:\n",
        "        for i in range(0, total, batch_size):\n",
        "            batch = triplets[i:i + batch_size]\n",
        "            tasks = []\n",
        "\n",
        "            for idx, triplet in enumerate(batch):\n",
        "                episode_body = {\n",
        "                    \"Subject\": triplet.get(\"Subject\"),\n",
        "                    \"Predicate\": triplet.get(\"Predicate\"),\n",
        "                    \"Object\": triplet.get(\"Object\")\n",
        "                }\n",
        "\n",
        "                tasks.append(\n",
        "                    client.add_episode(\n",
        "                        name=triplet.get('Subject', f'Triplet {i+idx}'),\n",
        "                        episode_body=json.dumps(episode_body),\n",
        "                        source_description='USDA KG Triplet',\n",
        "                        source=EpisodeType.json,\n",
        "                        reference_time=datetime.now(timezone.utc),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            try:\n",
        "                await asyncio.gather(*tasks)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in batch starting at {i}: {e}\")\n",
        "\n",
        "            pbar.update(len(batch))\n",
        "\n",
        "    logger.info(\"Finished ingesting all USDA triplets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhHqEFIvYnWe",
        "outputId": "a3e8b34c-8e3b-40e5-d234-7a674fd4b528"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingesting USDA Triplets: 100% |██████████|"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__main__ - INFO - Finished ingesting all USDA triplets.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "await ingest_usda_triplets(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfgunWZJYnWe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "user_name = 'health_user'\n",
        "\n",
        "# Create user node in Graphiti with a health-specific context\n",
        "await client.add_episode(\n",
        "    name='User Creation',\n",
        "    episode_body=f'{user_name} is interested in learning about nutrition and health topics.',\n",
        "    source=EpisodeType.text,\n",
        "    reference_time=datetime.now(timezone.utc),\n",
        "    source_description='HealthBot',\n",
        ")\n",
        "\n",
        "# Retrieve the created user's node UUID\n",
        "nl = await client._search(user_name, NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
        "user_node_uuid = nl.nodes[0].uuid if nl.nodes else None\n",
        "\n",
        "# Retrieve the HealthBot node UUID (optional)\n",
        "nl = await client._search('HealthBot', NODE_HYBRID_SEARCH_EPISODE_MENTIONS)\n",
        "healthbot_node_uuid = nl.nodes[0].uuid if nl.nodes else None\n",
        "\n",
        "# Utility function to convert entity edges to a clean facts string\n",
        "def edges_to_facts_string(entities: list[EntityEdge]) -> str:\n",
        "    return '-' + '\\n- '.join([edge.fact for edge in entities]) if entities else 'No facts found.'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EK8bYxsYnWf"
      },
      "source": [
        "# **DEFINE THE GET_HEALTH_DATA TOOL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMZVgfafYnWf",
        "outputId": "7bdea195-8b73-4c48-8a62-3213f876b23c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [ToolMessage(content='-Broccoli, raw contains Protein\\n- Broccoli, raw contains Vitamin K (Dihydrophylloquinone)\\n- Broccoli, raw contains Folate, total\\n- Broccoli, raw contains Sodium, Na\\n- Broccoli, raw contains Fatty acids, total saturated\\n- Broccoli, raw contains PUFA 18:2\\n- Broccoli, raw contains PUFA 18:3\\n- Broccoli, raw contains Thiamin\\n- Broccoli, raw contains SFA 14:0\\n- Broccoli, raw contains Vitamin B-6\\n- Broccoli, raw contains Vitamin K (phylloquinone)\\n- Broccoli, raw contains Vitamin E (alpha-tocopherol)\\n- Broccoli, raw contains Calcium, Ca\\n- Broccoli, raw contains Fiber, total dietary\\n- Broccoli, raw contains Proline\\n- Broccoli, raw contains Tocopherol, beta\\n- Broccoli, raw contains PUFA 20:4c\\n- Broccoli, raw contains Cryptoxanthin, beta\\n- Broccoli, raw contains Iron, Fe\\n- Broccoli, raw contains Phytofluene\\n- Broccoli, raw contains Pantothenic acid\\n- Broccoli, raw contains Carotene, beta\\n- Broccoli, raw contains Fiber, soluble\\n- Broccoli, raw contains Riboflavin\\n- Broccoli, raw contains Nitrogen', name='get_health_data', tool_call_id='call_iYi6JqVbDzTyQRoYMvcPl6tf')]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the health data retrieval tool\n",
        "@tool\n",
        "async def get_health_data(query: str) -> str:\n",
        "    \"\"\"Search the Graphiti graph for information about foods and nutrients.\"\"\"\n",
        "    edge_results = await client.search(\n",
        "        query,\n",
        "        center_node_uuid=healthbot_node_uuid,\n",
        "        num_results=25,\n",
        "    )\n",
        "    return edges_to_facts_string(edge_results)\n",
        "\n",
        "# Register the tool\n",
        "tools = [get_health_data]\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Initialize LLM and bind with tools\n",
        "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0.1).bind_tools(tools)\n",
        "\n",
        "# Test the tool node with a health query\n",
        "await tool_node.ainvoke({'messages': [await llm.ainvoke('nutrients in broccoli')]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anewtvaZYnWf"
      },
      "source": [
        "# **INITIALIZE JUDGEVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RZtrlq7YnWf"
      },
      "outputs": [],
      "source": [
        "judgment_tracer = Tracer(\n",
        "    api_key=\"...\",\n",
        "    project_name=\"graphiti-agent\",\n",
        "    organization_id=\"...\"\n",
        ")\n",
        "handler = JudgevalCallbackHandler(judgment_tracer)\n",
        "\n",
        "judgment_client = JudgmentClient(\n",
        "    judgment_api_key=\"...\",\n",
        "    organization_id=\"...\"\n",
        ")\n",
        "faithfulness_scorer = FaithfulnessScorer(threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tyxOiT5YnWf"
      },
      "source": [
        "# **THE CHATBOT FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuwrU448YnWf"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    user_name: str\n",
        "    user_node_uuid: str\n",
        "\n",
        "async def chatbot(state: State):\n",
        "    last_message = state['messages'][-1] if state.get('messages') else None\n",
        "    retrieval_context = []\n",
        "\n",
        "    # ===== Retrieve context from Graphiti =====\n",
        "    if last_message and isinstance(last_message, HumanMessage):\n",
        "        search_query = last_message.content\n",
        "        search_results = await client.search(\n",
        "            search_query,\n",
        "            num_results=25,\n",
        "        )\n",
        "        retrieval_context = [edge.fact for edge in search_results] if search_results else []\n",
        "\n",
        "    # System prompt\n",
        "    system_message = SystemMessage(\n",
        "    content=(\n",
        "        \"You are HealthBot, an expert clinical nutrition and health assistant with advanced knowledge of food science and biochemistry. \"\n",
        "        \"All your responses are grounded in authoritative databases such as the USDA FoodData Central and your extensive medical and nutritional expertise. \"\n",
        "        \"Provide clear, confident, and concise answers with relevant numeric details, standard ranges, and practical advice, as a professional nutrition specialist would communicate to patients or users. \"\n",
        "        \"If a specific value is not directly available in the context, integrate your domain knowledge to provide standard values or best-known estimates without mentioning lack of data or disclaimers. \"\n",
        "        \"Never reveal missing context or limitations in your knowledge. \"\n",
        "        \"Always maintain a trustworthy, direct, and professional tone, ensuring the user receives an actionable and confident response.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "    messages = [system_message] + state['messages']\n",
        "\n",
        "    # Get LLM response with tool binding\n",
        "    response = await llm.ainvoke(messages)\n",
        "\n",
        "    # Determine actual_output robustly\n",
        "    if hasattr(response, 'content') and response.content:\n",
        "        actual_output = response.content\n",
        "    elif hasattr(response, 'tool_calls'):\n",
        "        actual_output = f\"Tool call: {response.tool_calls}\"\n",
        "    else:\n",
        "        actual_output = str(response)\n",
        "\n",
        "    # Evaluate with Judgeval using retrieved context\n",
        "    if isinstance(last_message, HumanMessage):\n",
        "        example = Example(\n",
        "            input=last_message.content,\n",
        "            actual_output=actual_output,\n",
        "            retrieval_context=retrieval_context,\n",
        "        )\n",
        "        evaluation_results = judgment_client.run_evaluation(\n",
        "            examples=[example],\n",
        "            scorers=[faithfulness_scorer],\n",
        "            model=\"gpt-4o\",\n",
        "            project_name=\"graphiti-agent\",\n",
        "            eval_run_name=f\"health_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        )\n",
        "        print(\"\\nJudgeval Evaluation Results:\", evaluation_results)\n",
        "\n",
        "    return {'messages': [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6K7OoooYnWf"
      },
      "source": [
        "# **SET UP THE AGENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KERcmn1YYnWf"
      },
      "outputs": [],
      "source": [
        "async def should_continue(state, config):\n",
        "    last_message = state['messages'][-1]\n",
        "    if not last_message.tool_calls:\n",
        "        return 'end'\n",
        "    else:\n",
        "        return 'continue'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiQuFHYnYnWf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "memory = MemorySaver()\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node('agent', chatbot)\n",
        "graph_builder.add_node('tools', tool_node)\n",
        "\n",
        "graph_builder.add_edge(START, 'agent')\n",
        "graph_builder.add_conditional_edges('agent', should_continue, {'continue': 'tools', 'end': END})\n",
        "graph_builder.add_edge('tools', 'agent')\n",
        "\n",
        "graph = graph_builder.compile(checkpointer=memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUNNING THE AGENT FOR A SINGLE CALL**"
      ],
      "metadata": {
        "id": "NZJUlxfEZXe1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK1usvgCYnWg",
        "outputId": "484159d3-2e8f-46f2-bc9f-aeff1795fe74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your trace data here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/monitor?trace_id=9da8ba0f-8305-4e7f-8d58-09cb30ae3d2d&show_trace=true\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Trace</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your trace data here: \u001b]8;id=690657;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/monitor?trace_id=9da8ba0f-8305-4e7f-8d58-09cb30ae3d2d&show_trace=true\u001b\\\u001b[38;2;106;0;255mView Trace\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_121532\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=920613;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_121532\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Judgeval Evaluation Results: [ScoringResult(success=True, scorers_data=[ScorerData(name='Faithfulness', threshold=0.5, success=True, score=1.0, reason='The score is 1.00 because there are no contradictions between the output and the retrieval context. Everything is perfectly aligned and factually consistent. Great job!', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata={'claims': [{'claim': 'A tool call was made for getting health data on 1% Milk ingredients.', 'quote': \"Tool call: [{'name': 'get_health_data', 'args': {'query': '1% Milk ingredients'}, 'id': 'call_Rx7RSC9VKq0aQO22XngVshdO', 'type': 'tool_call'}\"}, {'claim': 'A tool call was made for getting health data on Broccoli nutrients.', 'quote': \"{'name': 'get_health_data', 'args': {'query': 'Broccoli nutrients'}, 'id': 'call_uConwqOzOMexgQodN5rCAFYr', 'type': 'tool_call'}\"}, {'claim': 'The tool call for 1% Milk ingredients has an ID of call_Rx7RSC9VKq0aQO22XngVshdO.', 'quote': \"Tool call: [{'name': 'get_health_data', 'args': {'query': '1% Milk ingredients'}, 'id': 'call_Rx7RSC9VKq0aQO22XngVshdO', 'type': 'tool_call'}\"}, {'claim': 'The tool call for Broccoli nutrients has an ID of call_uConwqOzOMexgQodN5rCAFYr.', 'quote': \"{'name': 'get_health_data', 'args': {'query': 'Broccoli nutrients'}, 'id': 'call_uConwqOzOMexgQodN5rCAFYr', 'type': 'tool_call'}\"}, {'claim': \"The tool call for 1% Milk ingredients is of type 'tool_call'.\", 'quote': \"Tool call: [{'name': 'get_health_data', 'args': {'query': '1% Milk ingredients'}, 'id': 'call_Rx7RSC9VKq0aQO22XngVshdO', 'type': 'tool_call'}\"}, {'claim': \"The tool call for Broccoli nutrients is of type 'tool_call'.\", 'quote': \"{'name': 'get_health_data', 'args': {'query': 'Broccoli nutrients'}, 'id': 'call_uConwqOzOMexgQodN5rCAFYr', 'type': 'tool_call'}\"}], 'verdicts': [{'verdict': 'idk', 'reason': 'The retrieval context only mentions the contents of 1% Milk and does not provide any information regarding a tool call for getting health data on 1% Milk ingredients.'}, {'verdict': 'idk', 'reason': 'The retrieval context does not mention any tool call nor does it discuss anything related to Broccoli nutrients.'}, {'verdict': 'idk', 'reason': 'The retrieval context does not provide any information about tool calls or their identification numbers.'}, {'verdict': 'idk', 'reason': 'The retrieval context contains no information about tool calls or their identification numbers.'}, {'verdict': 'idk', 'reason': 'There is no information in the retrieval context about the type of tool call for 1% Milk ingredients.'}, {'verdict': 'idk', 'reason': 'The retrieval context does not discuss any tool calls, types, or anything related to Broccoli nutrients.'}]})], name=None, data_object=Example(input='What are the ingredients in 1% Milk? And what does Broccoli contain?', actual_output=\"Tool call: [{'name': 'get_health_data', 'args': {'query': '1% Milk ingredients'}, 'id': 'call_Rx7RSC9VKq0aQO22XngVshdO', 'type': 'tool_call'}, {'name': 'get_health_data', 'args': {'query': 'Broccoli nutrients'}, 'id': 'call_uConwqOzOMexgQodN5rCAFYr', 'type': 'tool_call'}]\", expected_output=None, context=None, retrieval_context=['MILK, 1% contains Water', 'MILK, 1% contains Water', 'MILK, 1% contains Water', 'MILK, 1% contains Water', 'MILK, 1% contains Water', 'MILK, 1% contains Methionine', 'MILK, 1% contains Methionine', 'MILK, 1% contains Methionine', 'MILK, 1% contains Methionine', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Vitamin D3 (cholecalciferol)', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'MILK, 1% contains Cholesterol', 'Milk, 1% contains Lactose', 'Milk, 1% contains Lactose', 'Milk, 1% contains Lactose'], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id='964cfcd3-f742-4dc7-bcc9-4312f89d8240', example_index=None, created_at='2025-07-11T12:15:32.219817', trace_id=None), trace_id=None, run_duration=None, evaluation_cost=None)]\n",
            "\n",
            "Assistant: ### Ingredients in 1% Milk:\n",
            "1% Milk primarily contains:\n",
            "- Water\n",
            "- Nitrogen\n",
            "- Cholesterol\n",
            "- Glucose\n",
            "- Vitamin D3 (cholecalciferol)\n",
            "- Maltose\n",
            "- Methionine\n",
            "- Threonine\n",
            "- Glycine\n",
            "- Total Fatty Acids (including TFA 20:1)\n",
            "\n",
            "### Nutritional Components of Broccoli (Raw):\n",
            "Broccoli is rich in various nutrients, including:\n",
            "- Protein\n",
            "- Folate (total)\n",
            "- Vitamin K (both Dihydrophylloquinone and phylloquinone)\n",
            "- Sodium\n",
            "- Total saturated fatty acids\n",
            "- Thiamin\n",
            "- Vitamin B-6\n",
            "- Calcium\n",
            "- Vitamin E (alpha-tocopherol)\n",
            "- Total dietary fiber (including soluble fiber)\n",
            "- Iron\n",
            "- Various fatty acids (including PUFA 18:2, 18:3, and 20:4c)\n",
            "- Riboflavin\n",
            "- Pantothenic acid\n",
            "- Phytofluene\n",
            "- Cryptoxanthin (beta)\n",
            "\n",
            "Broccoli is an excellent source of vitamins and minerals, making it a highly nutritious vegetable choice.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Example test input\n",
        "test_input = {\n",
        "    'messages': [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'What are the ingredients in 1% Milk? And what does Broccoli contain?'\n",
        "        }\n",
        "    ],\n",
        "    'user_name': 'health_user',\n",
        "    'user_node_uuid': None,\n",
        "}\n",
        "\n",
        "# Config with Judgeval handler and thread_id\n",
        "config = {\n",
        "    'configurable': {'thread_id': uuid.uuid4().hex},\n",
        "    'callbacks': [handler],\n",
        "}\n",
        "\n",
        "# Run the agent graph with this input\n",
        "final_state = await graph.ainvoke(test_input, config=config)\n",
        "\n",
        "# Print the output assistant message\n",
        "if 'messages' in final_state and isinstance(final_state['messages'], list):\n",
        "    last_message = final_state['messages'][-1]\n",
        "    print(\"\\nAssistant:\", last_message.content if hasattr(last_message, \"content\") else last_message)\n",
        "else:\n",
        "    print(\"No messages returned.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV94FoltYnWg"
      },
      "source": [
        "# **RUNNING THE AGENT IN INTERACTIVE MODE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6e08406d6779415e9002a35e87e6872a"
          ]
        },
        "id": "L1n72EssYnWg",
        "outputId": "9d2d7570-558f-4075-ed6a-9380476167bb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e08406d6779415e9002a35e87e6872a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Text(value='', placeholder='Type your message here...'), Button(description='Send', style=Butto…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_122319\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=451809;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_122319\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_122814\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=49277;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_122814\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_123441\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=349094;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=health_eval_20250711_123441\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Configure conversation output widget\n",
        "conversation_output = widgets.Output()\n",
        "config = {'configurable': {'thread_id': uuid.uuid4().hex}}\n",
        "\n",
        "user_state = {'user_name': user_name}\n",
        "\n",
        "# Define process_input to handle user messages\n",
        "async def process_input(user_state: State, user_input: str):\n",
        "    conversation_output.append_stdout(f'\\nUser: {user_input}\\n')\n",
        "    conversation_output.append_stdout('\\nAssistant: ')\n",
        "\n",
        "    graph_state = {\n",
        "        'messages': [{'role': 'user', 'content': user_input}],\n",
        "        'user_name': user_state['user_name'],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        async for event in graph.astream(\n",
        "            graph_state,\n",
        "            config=config,\n",
        "        ):\n",
        "            for value in event.values():\n",
        "                if 'messages' in value:\n",
        "                    last_message = value['messages'][-1]\n",
        "                    if isinstance(last_message, AIMessage) and hasattr(last_message, 'content'):\n",
        "                        conversation_output.append_stdout(last_message.content)\n",
        "    except Exception as e:\n",
        "        conversation_output.append_stdout(f'Error: {e}')\n",
        "\n",
        "# Setup input box and submit button\n",
        "def on_submit(b):\n",
        "    user_input = input_box.value\n",
        "    input_box.value = ''\n",
        "    asyncio.create_task(process_input(user_state, user_input))\n",
        "\n",
        "input_box = widgets.Text(placeholder='Type your message here...')\n",
        "submit_button = widgets.Button(description='Send')\n",
        "submit_button.on_click(on_submit)\n",
        "\n",
        "# Display initial greeting and UI\n",
        "conversation_output.append_stdout('Assistant: Hello, how can I help you today?')\n",
        "\n",
        "display(widgets.VBox([input_box, submit_button, conversation_output]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN1uJmJ9YnWg"
      },
      "source": [
        "# **INSTRUCTION ADHERENCE UNIT TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X0cDgELYnWg",
        "outputId": "2c9fb97f-b40e-49a7-92e3-0851bed6f14d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=instruction_adherence_test\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=114913;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=instruction_adherence_test\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unit test evaluation results:\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Instruction Adherence', threshold=0.8, success=True, score=1.0, reason=\"[Verdict(instruction='1. List three key nutrients found in spinach.', analysis='The LLM output correctly listed three key nutrients: Iron, Vitamin K, and Folate.', score=1.0), Verdict(instruction='2. Identify and specify the nutrient that is highest in quantity per 100 grams of spinach.', analysis='The LLM correctly identified Vitamin K as the nutrient that is highest in quantity per 100 grams of spinach.', score=1.0)]\", strict_mode=None, evaluation_model='gpt-4o-mini', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=List three key nutrients found in spinach. Then tell me which one is highest in quantity per 100 grams., actual_output=1. Iron\n",
            "2. Vitamin K\n",
            "3. Folate\n",
            "Among these, Vitamin K is highest in quantity per 100 grams of spinach., expected_output=None, context=None, retrieval_context=None, additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=64ca733a-526e-48bc-bfb7-39215456e8d0, example_index=None, created_at=2025-07-11T17:23:03.420347, ,             run_duration=None,             evaluation_cost=None)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "example = Example(\n",
        "    input=\"List three key nutrients found in spinach. Then tell me which one is highest in quantity per 100 grams.\",\n",
        "    actual_output=(\n",
        "        \"1. Iron\\n\"\n",
        "        \"2. Vitamin K\\n\"\n",
        "        \"3. Folate\\n\"\n",
        "        \"Among these, Vitamin K is highest in quantity per 100 grams of spinach.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# ============ Initialize InstructionAdherenceScorer ============\n",
        "scorer = InstructionAdherenceScorer(threshold=0.8)\n",
        "\n",
        "# ============ Run unit test evaluation ============\n",
        "results = client.run_evaluation(\n",
        "    examples=[example],\n",
        "    scorers=[scorer],\n",
        "    model=\"gpt-4o-mini\",\n",
        "    project_name=\"graphiti-agent\",\n",
        "    eval_run_name=\"instruction_adherence_test\"\n",
        ")\n",
        "\n",
        "# ============ Print results ============\n",
        "print(\"Unit test evaluation results:\")\n",
        "for r in results:\n",
        "    print(r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PPmKswcYnWh"
      },
      "source": [
        "# **EVALUATION OF THE DATASET ON THE NUTRIBENCH BENCHMARK DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NutriBench is a benchmark dataset designed to evaluate nutrition QA sets. It contains realistic meal descriptions paired with nutritional information such as carbohydrates, protein, fat, and energy values. The dataset combines natural language meal entries with structured nutrient data to test models on understanding, retrieval, and reasoning in nutrition-related tasks. It is valuable for developing and benchmarking healthbots, diet planners, and food analysis systems."
      ],
      "metadata": {
        "id": "JqZgCkN0aeNX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Atek-Cz9YnWh",
        "outputId": "bd164b26-6efe-4be1-8759-34943ba8131e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating examples: 100%|██████████| 50/50 [01:06<00:00,  1.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation with Judgeval...\n",
            "                     \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "🔍 You can view your evaluation results here: <a href=\"https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=nutribench_correctness_eval_20250711_172542\" target=\"_blank\"><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">View Results</span></a>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "🔍 You can view your evaluation results here: \u001b]8;id=117662;https://app.judgmentlabs.ai/app/project/a3d40e26-c2af-43e7-85be-f30132e92882/experiment?eval_run_name=nutribench_correctness_eval_20250711_172542\u001b\\\u001b[38;2;106;0;255mView Results\u001b[0m\u001b]8;;\u001b\\\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation complete. Results summary:\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For a quick snack, I reached for 360g of bottled water, keeping it simple and hydrating., actual_output=Bottled water is an excellent choice for hydration, as it contains no calories, sugars, or fats. Drinking 360 grams (approximately 360 mL) of water provides essential hydration without any additional nutrients or calories. \n",
            "\n",
            "For optimal hydration, aim to drink water throughout the day, especially if you're active or in a hot environment. The general recommendation is about 2 to 3 liters (or 8 to 12 cups) of water daily, depending on individual needs, activity level, and climate. Your choice of bottled water is a great way to meet those hydration needs., expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=00e3a8b6-4fcf-41c5-8632-dedba641f284, example_index=None, created_at=2025-07-11T17:24:37.871837, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output, and there were no incorrect or inconsistent statements identified. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=During lunch, I savored 240g of unsweetened bottled water, 142g of thin crust pepperoni pizza from school, and a delightful 248g of ready-to-drink reduced sugar chocolate milk., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=7bb60978-361a-4d90-9cd9-bf6bbd20bbda, example_index=None, created_at=2025-07-11T17:24:39.537426, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=False,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=False, score=0.0, reason='The score is 0.00 because both the expected and the actual outputs are empty, leading to no verified correct statements or meaningful comparisons. As a result, there is no content to score, justifying the lowest possible score.', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I indulged in a delicious 75g vanilla ice cream bar coated in chocolate., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=76b9f918-b4f8-42d0-a58e-ac873a6ca539, example_index=None, created_at=2025-07-11T17:24:41.072397, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason='The score is 1.00 because the model output perfectly matched the expected output, showing no incorrect or inconsistent statements. Great job!', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I kicked off my day with a satisfying breakfast featuring 135g of cheesy egg omelet made with oil, alongside 20g of flavorful pork sausage, a serving of 60g plain French toast from frozen, and a sweet addition of 15g light pancake syrup., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=cd5e5a51-e4c6-4ddf-8edc-b5e6f2837924, example_index=None, created_at=2025-07-11T17:24:43.119010, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements, demonstrating excellent accuracy and consistency.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My afternoon snack featured a 200g raw apple and a crunchy 28g serving of cheese-flavored popcorn., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=c98efe85-6b8e-4797-b153-98aaf994770d, example_index=None, created_at=2025-07-11T17:24:44.898970, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For a quick snack, I opted for 186g of fruit juice drink, which is loaded with high vitamin C., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=4758c6cf-0e49-472f-b322-3e60feef8231, example_index=None, created_at=2025-07-11T17:24:45.782024, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=False,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=False, score=0.0, reason='The score is 0.00 because the actual output is empty and provides no information to confirm or deny any statements, resulting in complete inconsistency with the expected output.', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My lunch included a tasty 112g peanut butter and jelly sandwich, with a side of 248g whole strawberry milk for a sweet touch., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=ce7f8ee9-a671-4609-86fc-8f7bf625e1d3, example_index=None, created_at=2025-07-11T17:24:46.908491, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no inconsistencies or incorrect statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=During my snack time, I treated myself to 524g of cola., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=94ce2afb-a1c1-4538-b982-4ffbeb1e53b7, example_index=None, created_at=2025-07-11T17:24:48.049585, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly aligned with the expected output, showing no incorrect or inconsistent statements. Great job maintaining accuracy and consistency throughout!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My lunch featured 35g sauteed chicken wings, 60g of skin-on sautéed chicken drumsticks, paired with 250g of fluffy mashed potatoes and a generous 17g serving of ketchup., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=f7961b23-80f3-44ef-8d2c-637be9d7bbda, example_index=None, created_at=2025-07-11T17:24:49.741115, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=False,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=False, score=0.0, reason=\"The score is 0.00 because the expected output was completely unaddressed, containing only the word 'Text:' which is absent or not referenced in the actual output. Without any matched or relevant content, the answer completely fails to align with the expected result.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I started my day with 70g of pan dulce, which had a lovely sugar topping, paired with a warm 240g cup of reconstituted coffee., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=9b0c5116-8213-4145-b63e-5ef31f1b2285, example_index=None, created_at=2025-07-11T17:24:51.268496, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For a quick snack, I had a delicious 165g raw apple paired with 240g of refreshing unsweetened bottled water., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=729be520-8565-49f4-96f8-813af8a8fbac, example_index=None, created_at=2025-07-11T17:24:52.428431, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=During my snack time, I enjoyed a refreshing 200g apple straight from the fruit bowl., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=20ecfbde-fd77-4b0c-a312-429dbb814718, example_index=None, created_at=2025-07-11T17:24:53.686424, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output is perfectly accurate and consistent with the expected output, showcasing an excellent understanding of the task.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=This evening, my dinner consisted of 85g of deliciously fried chicken drumstick, 160g of restaurant-style cooked broccoli, and 55g of flavorful fried chicken wings., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=6931d4b0-d294-4722-81d4-df78073162de, example_index=None, created_at=2025-07-11T17:24:55.295216, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no inconsistencies or errors. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=This evening's dinner included a 75g chocolate doughnut, paired with a refreshing 240g of unsweetened bottled water, and a creamy 245g serving of Greek low-fat yogurt with fruit., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=789a530f-719a-470e-9083-79bda6765981, example_index=None, created_at=2025-07-11T17:24:58.045838, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements. Excellent work!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I had 239g of delicious chicken or turkey vegetable soup from my own recipe for lunch., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=f6114f0f-7282-4f99-b318-b5bc8d778d8b, example_index=None, created_at=2025-07-11T17:24:58.826894, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Excellent job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=This morning, I treated myself to a 130g chocolate muffin paired with a refreshing 124g glass of 100% apple juice., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=999a5b4b-a60e-4161-ad1d-0f8f71a08fc5, example_index=None, created_at=2025-07-11T17:25:00.173420, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no inconsistent or incorrect statements. Excellent job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For lunch, I had a delicious 150g low-fat yogurt parfait with fruit and 240g of bottled unsweetened water., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=81b5670b-2a6c-4675-8785-cf52495ff16d, example_index=None, created_at=2025-07-11T17:25:01.457876, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason='The score is 1.00 because the output perfectly matches the expected response with no incorrect statements. Great job!', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my breakfast, I poured myself 244g of NFS milk to kickstart the day., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=1dc261f4-43ae-4bba-9af1-8b9ce5fa95d1, example_index=None, created_at=2025-07-11T17:25:02.481137, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Excellent job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to 147g of delicious school lunch pizza topped with pepperoni for lunch today., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=292c63a7-5e39-44e3-9ff2-0c2ccabbe41f, example_index=None, created_at=2025-07-11T17:25:03.198760, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I kicked off my breakfast with a 126g raw banana, a refreshing 720g of tap water, and a delightful 128g egg, cheese, and sausage on a biscuit., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=c6230f65-3f57-49c3-9ad8-70e33bbf26c6, example_index=None, created_at=2025-07-11T17:25:04.688539, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no incorrect or inconsistent statements.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a 54g breakfast tart this morning, and it was a lovely way to begin my day., actual_output=That sounds delightful! Breakfast tarts can be a tasty way to start the day. If you're interested, I can provide nutritional information about breakfast tarts or specific ingredients commonly found in them. Just let me know what you'd like to know!, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=b9e64b2d-bb2f-4314-91f0-8a967b841c20, example_index=None, created_at=2025-07-11T17:25:06.053877, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements, demonstrating an excellent understanding of the task.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=Today’s lunch included a delicious 115g hamburger with a single patty on a soft white bun, along with 17g of ketchup for added flavor., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=e1a16b88-5747-4382-9142-e264375c8644, example_index=None, created_at=2025-07-11T17:25:07.078609, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a 372g serving of ginger ale as a delightful snack., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=e3e6ae22-d7de-4db6-8ce4-b2b93186ff85, example_index=None, created_at=2025-07-11T17:25:07.899964, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason='The score is 1.00 because the outputs are perfectly consistent and match exactly as expected. There are no incorrect or inconsistent statements, indicating a flawless performance.', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had 244g of reduced fat (2%) milk., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=09e81660-7ee1-4f9a-902b-ea544848dc0f, example_index=None, created_at=2025-07-11T17:25:08.667034, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no incorrect or inconsistent statements. Excellent work!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had 215g of Easy Mac-style macaroni and cheese alongside 70g of baked chicken thigh, skin removed., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=96b7492f-90b2-4740-9c0c-7c8e77ebdbc3, example_index=None, created_at=2025-07-11T17:25:09.898847, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My lunch consisted of 244g of reduced-fat milk, 105g of delicious baked chicken breast with the skin removed, a glass of 240g of tap water, and a generous 250g portion of school lunch mashed potatoes., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=37cd9405-81a1-471f-bd57-ed70706ee601, example_index=None, created_at=2025-07-11T17:25:11.620387, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no inconsistencies or incorrect statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For breakfast today, I savored a 40g frozen chocolate waffle with a generous 20g serving of pancake syrup., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=ef044aeb-63a8-433e-a54a-8d22444ae24d, example_index=None, created_at=2025-07-11T17:25:12.694323, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no inconsistencies or incorrect statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=Today for lunch, I had a generous 250g serving of regular applesauce and a tasty 56g peanut butter and jelly sandwich on wheat bread., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=b5502f94-d221-42fb-a80b-51d4910e3479, example_index=None, created_at=2025-07-11T17:25:13.967714, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Great job maintaining accuracy!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had 240g of tap water., actual_output=Tap water contains no calories, fats, carbohydrates, or proteins. It is primarily composed of water (H2O) and may contain trace minerals depending on the source. Drinking water is essential for hydration and supports various bodily functions, including temperature regulation, nutrient transport, and waste elimination.\n",
            "\n",
            "For optimal hydration, it's generally recommended to drink about 2 to 3 liters (or 8 to 12 cups) of water per day, depending on factors like activity level, climate, and individual health needs. Your 240g of tap water is approximately equivalent to 240 mL, which contributes to your daily hydration needs., expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=5ecc46b1-d206-4195-b886-8ba838930170, example_index=None, created_at=2025-07-11T17:25:15.611127, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a snack consisting of 150g of fresh raw grapes and a delightful 50g freezer pop., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=77a99b8f-6b59-4500-aa85-4e206f8383bc, example_index=None, created_at=2025-07-11T17:25:16.779990, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For breakfast today, I enjoyed 57g of delicious plain ruffled potato chips., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=9ca74053-b2c8-4a6c-b75a-76076f725047, example_index=None, created_at=2025-07-11T17:25:17.728900, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=False,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=False, score=0.0, reason='The score is 0.00 because the actual output is empty, meaning it provides no alignment with the expected output, and therefore cannot confirm any of the required statements or facts.', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a sweet 200g raw apple and a generous 496g of fruit juice drink for my snack today., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=71a22cc2-b1b3-492c-acb1-489e9e5728b7, example_index=None, created_at=2025-07-11T17:25:18.853780, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Excellent job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My snack consisted of a satisfying 524g of a caffeine-containing fruit-flavored soft drink., actual_output=To provide you with relevant nutritional information about your fruit-flavored soft drink, I will look up the typical values for caffeine content and other nutritional components in a standard serving of such beverages. Please hold on for a moment., expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=013d2d34-99e2-459b-9d96-fc52d342c0b0, example_index=None, created_at=2025-07-11T17:25:20.562112, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output, with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=During my afternoon snack, I enjoyed a refreshing 360g bottle of unsweetened water paired with a nutritious 68g Clif Bar., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=456624ee-03bb-47e1-a739-227d6e1c9e3d, example_index=None, created_at=2025-07-11T17:25:22.850288, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements present. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to 28 grams of plain baked potato chips for my snack., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=956ba265-3afd-4330-8159-e9aed901df54, example_index=None, created_at=2025-07-11T17:25:23.567352, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no inconsistencies or incorrect statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a 154g raw orange during my snack time, and it was deliciously sweet., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=c13cd752-965e-4242-9e02-a33b2b185c4c, example_index=None, created_at=2025-07-11T17:25:24.182808, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=False,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=False, score=0.0, reason=\"The score is 0.00 because the actual output is empty and provides no information, making it impossible to verify any statements from the expected output. Therefore, the model's output does not match the expected output at all, justifying the score of 0.00.\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to 28 grams of crunchy plain potato chips for a quick snack., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=f5c2ef5e-cdf7-4272-aeef-347153812e56, example_index=None, created_at=2025-07-11T17:25:25.115554, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output without any inconsistencies or errors. Great job on achieving complete accuracy!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For a light snack, I opted for an 80g frozen fruit juice bar., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=340879c3-cbcf-47d4-992a-212651433f02, example_index=None, created_at=2025-07-11T17:25:25.944468, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output, with no incorrect or inconsistent statements identified. Great job on achieving a flawless performance!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My dinner consisted of a 240g glass of tap water and a hearty 134g beef steak, which was both breaded and fried to perfection., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=6e9bca4d-75ed-493b-b5b5-41f98aa0abba, example_index=None, created_at=2025-07-11T17:25:27.457060, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because there are no incorrect statements, indicating that the model's output perfectly matches the expected output. Great job maintaining accuracy and consistency!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For breakfast, I had a delightful 73g croissant paired with 240g of brewed coffee., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=6a3b5447-31e2-4fba-ba4e-cea40c3ef060, example_index=None, created_at=2025-07-11T17:25:28.940511, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason='The score is 1.00 because the output perfectly matches the expected output with no incorrect or inconsistent statements. Great job!', strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my lunch, I had a comforting 370g of noodle soup and washed it down with 240g of tap water., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=8b9282b5-5ad6-4c6d-9db9-8810df77eb33, example_index=None, created_at=2025-07-11T17:25:30.939352, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had a hearty 370g bowl of soup, mostly filled with noodles., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=3d1b768b-afa2-472b-b5e9-e73b5566a06a, example_index=None, created_at=2025-07-11T17:25:31.962388, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had 240g of unsweetened bottled baby water., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=05d0c9b1-d47c-410e-9ce2-850587f22858, example_index=None, created_at=2025-07-11T17:25:32.882665, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements. Great job on achieving a flawless result!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My lunchtime meal consisted of a 92g peanut butter sandwich made with regular peanut butter on white bread, along with a generous serving of 244g whole milk., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=2ce01d7b-c386-4f22-bd1d-09d1b76c5d7f, example_index=None, created_at=2025-07-11T17:25:34.606568, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no incorrect or inconsistent statements identified. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=For my snack, I had 50g of freezer pop along with 26g of fruit leather and fruit snacks candy., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=053221a9-705c-4c86-a6eb-82db189d7282, example_index=None, created_at=2025-07-11T17:25:36.410265, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output without any incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=My breakfast routine includes a satisfying 78g Kashi Chewy cereal bar complemented by 360g of robust brewed coffee., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=56c669c3-72f4-4694-a61a-20c90fc0ffcb, example_index=None, created_at=2025-07-11T17:25:37.699101, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output with no incorrect or inconsistent statements. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=During dinner, I had 10g of beef sausage, complemented by 360g of diet cola and a buttery 28g croissant., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=46065cf7-cfca-46c3-ba5b-f8eb0e6c7da2, example_index=None, created_at=2025-07-11T17:25:39.638031, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matched the expected output, with no incorrect or inconsistent statements identified. Great job!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I treated myself to a delightful snack consisting of 30g of flavored fat-free coffee creamer added to 480g of freshly brewed coffee., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=572d5b51-837a-403d-82b4-463245aecb08, example_index=None, created_at=2025-07-11T17:25:41.002237, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output with no incorrect or inconsistent statements. Great job on achieving complete accuracy!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=I enjoyed 244g of whole milk as a refreshing snack., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=f4687d25-0561-4ac6-8fdf-fcc084c1d1a8, example_index=None, created_at=2025-07-11T17:25:41.795285, ,             run_duration=None,             evaluation_cost=None)\n",
            "ScoringResult(            success=True,             scorer_data=[ScorerData(name='Answer Correctness', threshold=0.8, success=True, score=1.0, reason=\"The score is 1.00 because the model's output perfectly matches the expected output, with no incorrect or inconsistent statements identified. Great job maintaining complete accuracy!\", strict_mode=None, evaluation_model='gpt-4o', error=None, evaluation_cost=None, verbose_logs=None, additional_metadata=None)],             data_object=Example(input=This evening, my snack consisted of 57g of ruffled potato chips, bursting with flavor., actual_output=, expected_output=, context=None, retrieval_context=[], additional_metadata=None, tools_called=None, expected_tools=None, name=None, example_id=bce0bf33-69fe-4bd8-b6a2-a8f158bb0ebf, example_index=None, created_at=2025-07-11T17:25:42.600046, ,             run_duration=None,             evaluation_cost=None)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ============ Load NutriBench CSV benchmark ============\n",
        "df = pd.read_csv(\"NutriBench.csv\").head(50)\n",
        "\n",
        "\n",
        "correctness_scorer = AnswerCorrectnessScorer(threshold=0.8)\n",
        "\n",
        "# ============ Initialize examples list ============\n",
        "examples = []\n",
        "\n",
        "async def generate_examples():\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating examples\"):\n",
        "        user_input = row[\"meal_description\"]\n",
        "\n",
        "        # Build agent state for your HealthBot\n",
        "        state = {\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
        "            \"user_name\": \"benchmark_user\",\n",
        "            \"user_node_uuid\": \"benchmark_uuid\"\n",
        "        }\n",
        "\n",
        "        # Call your chatbot agent function\n",
        "        bot_response = await chatbot(state)\n",
        "\n",
        "        # Build Example object\n",
        "        example = Example(\n",
        "            input=user_input,\n",
        "            actual_output=bot_response['messages'][0].content,\n",
        "            expected_output=row.get(\"expected_output\", \"\"),\n",
        "            retrieval_context=[],  # add retrieval context if available\n",
        "        )\n",
        "        examples.append(example)\n",
        "\n",
        "# ============ Main entrypoint ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the asynchronous example generation with progress bar\n",
        "    asyncio.run(generate_examples())\n",
        "\n",
        "    # Build EvalDataset\n",
        "    dataset = EvalDataset(examples=examples)\n",
        "\n",
        "    # Evaluate the dataset using AnswerCorrectnessScorer\n",
        "    print(\"Running evaluation with Judgeval...\")\n",
        "    results = client.run_evaluation(\n",
        "        examples=dataset.examples,\n",
        "        scorers=[correctness_scorer],\n",
        "        model=\"gpt-4o\",\n",
        "        project_name=\"graphiti-agent\",\n",
        "        eval_run_name=f\"nutribench_correctness_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    )\n",
        "\n",
        "    # Print results summary\n",
        "    print(\"Evaluation complete. Results summary:\")\n",
        "    for r in results:\n",
        "        print(r)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
